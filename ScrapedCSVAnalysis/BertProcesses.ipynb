{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8c3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "loadmodel = TFBertForSequenceClassification.from_pretrained(\"models/SentModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "467a1386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
    "    train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    return train_InputExamples, validation_InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6adf246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce3c765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(list_preds):\n",
    "    tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "    tf_outputs = loadmodel(tf_batch)\n",
    "    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "    outputs = tf_predictions.numpy()\n",
    "    abs_outs = []\n",
    "    for i_output in outputs:\n",
    "        abs_outs.append(i_output[-1])\n",
    "    return abs_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86c7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
