{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob won't need all these but this is just my copy paste\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm, trange, tnrange\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and test it\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=5,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(device)\n",
    "\n",
    "model.load_state_dict(torch.load('finetuned_BERT_FairyGarbHVCHECK_epoch_3.model', map_location=torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "\n",
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_arr_in):\n",
    "    encoded_data = tokenizer.batch_encode_plus(\n",
    "    text_arr_in, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=MAX_LENGTH, \n",
    "    truncation = True,\n",
    "    return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIDs(encoded_data_in):\n",
    "    input_ids_train = encoded_data_in['input_ids']\n",
    "    attention_masks_train = encoded_data_in['attention_mask']\n",
    "    \n",
    "    return input_ids_train, attention_masks_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_bert(text_in):\n",
    "  encodings = encode(text_in)\n",
    "  inputIDS, attention_masks = getIDs(encodings)\n",
    "  return inputIDS, attention_masks\n",
    "\n",
    "# encodings = encode(sentences)\n",
    "\n",
    "# inputIDs, attention_masks = getIDs(encodings)\n",
    "# print(inputIDs[0])\n",
    "# print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mangid, mangmask = preproc_bert(mangost_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l is the inputIDs and attention masks you are feeding in\n",
    "# n is the number of chunks you want to output (I use 25)\n",
    "def divide_chunks(l, n):\n",
    "  # looping till length l\n",
    "  for i in range(0, len(l), n): \n",
    "      yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 32\n",
    "mango_inchunk = list(divide_chunks(mangid, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_text(input_chunk_in, att_chunk_in):\n",
    "  pred_arr = []\n",
    "\n",
    "  for i in range(len(input_chunk_in)):\n",
    "    input = input_chunk_in[i]\n",
    "    att = att_chunk_in[i]\n",
    "    input = input.to(device)\n",
    "    att = att.to(device)\n",
    "    output = model(input, att, return_dict=False)\n",
    "    _,prediction = torch.max(output[0], dim = 1)\n",
    "    pred_arr.append(prediction.tolist())\n",
    "\n",
    "  return pred_arr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc22371f3870926dbbcdf04161196d7334d8dc09d0b069ea32b843c425bf39e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
